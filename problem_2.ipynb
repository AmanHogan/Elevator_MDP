{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.a.i - 2.a.iii**\n",
    "\n",
    "**Q-learning** has been implemented and the graphs for 2.i - 2.iii can be viewed [HERE](./sample_output/q/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.b.i - 2.b.iii**\n",
    "\n",
    "**SARSA-learning** has been implemented and the graphs for 2.i - 2.iii can be viewed [HERE](./sample_output/sarsa/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.c.i Q Learning VS SARSA Learning**\n",
    "The main difference between Q learning and SARSA are how the Q table is updated after each action. So we dont notice too many differences. In terms of average rewards accross all alphas, gammas, and epsilons for both Q learning and SARSA, there was not much of a difference. However, in terms of average passenger wait times, there was a difference. In SARSA, there was much less variance between the average wait times across the changing alpha, gamma, and epsilon values. But in Q learning, there was a great variance between the avergae wait times for the changing variables. \n",
    "\n",
    "For example, notice how different in the variance in values for Q Learning and SARSA for alpha values, respectively:\n",
    "\n",
    "<img src=\"./sample_output/q/avg_times_alpha_2i.png\" width=\"400\"/>\n",
    "<img src=\"./sample_output/sarsa/avg_times_alpha_2i.png\"  width=\"400\"/>\n",
    "\n",
    "In SARSA, the values are closer to each other than in Q Learning, and this is reflected for both gamma and epsilon. Overall, there was not much of a difference between the end result of the best combinations of values for Q learning and SARSA learning.\n",
    "\n",
    "## **2.c.i Alpha VS Exploration VS GAMMA**\n",
    "\n",
    "For both Q learning and SARSA learning, changing the exploration rate had yielded the highest avergae rewards, with an averge reward of 5. And there was also a high variance among the average rewards when changing the exploration rates. Here is an the exploration rate graph for avg rewards for Q learning: \n",
    "\n",
    "<img src=\"./sample_output/q/avg_rewards_explore_2i.png\" width=\"400\"/>\n",
    "\n",
    "\n",
    "For the gamma and alpha values, they eventually all seemed to converge to the same value for both SARSA and Q learning. Here are the average rewards for changing gamma and alpha in Q learning:\n",
    "\n",
    "<img src=\"./sample_output/q/avg_rewards_alpha_2i.png\" width=\"400\"/>\n",
    "<img src=\"./sample_output/q/avg_rewards_gamma_2i.png\" width=\"400\"/>\n",
    "\n",
    "\n",
    "Overall, it seemed the best learning rate, discount factor, and exploration rate for both Q learning and SARSA was learning rate = .01, discount factor = .01, and exploration rate = .5 over 50,000 iterations. This means our agent needs to learn slow, care only about immediate rewards, and choose other actions 50% of the time, over 13.89 hours for it to take an average of about 3.25 minutes for the elevator to arrive to a person's call floor and take them to their exit floor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.c.ii Q Learning VS SARSA Learning**\n",
    "\n",
    "There was not much of a difference between rewards. Results were similar to 2.i; SARSA had a higher variance in wait times than Q learning.\n",
    "\n",
    "## **2.c.ii Alpha VS Exploration VS GAMMA**\n",
    "Changing the exploration rate still yielded the highest rewards and their is still not  much of a difference between the avg rewards for changing the gamma and changing the alpha. However, now, having a higher discount factor lowers the wait time. Here is what the avg wait time graph looks like for Q learning and SARSA with discount factors, respectively.\n",
    "\n",
    "<img src=\"./sample_output/q/avg_times_gamma_2ii.png\" width=\"400\"/>\n",
    "<img src=\"./sample_output/sarsa/avg_times_gamma_2ii.png\" width=\"400\"/>\n",
    "\n",
    "\n",
    "Overall, we see that the system at best preferred an alpha = .5 for Q, .01 for SARSA, gamma = .5 for Q, 1 for SARSA, and epsilon = .3 for Q, .5 for SARSA. This means we preffered an agent that learned at a modereate rate, cared about future actions, and stuck with actions that yielded high rewards over 13.89 hours for it to take an average of about 4 minutes for the elevator to arrive to a person's call floor and take them to their exit floor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.c.iii Q Learning VS SARSA Learning**\n",
    "There was not much of a difference between avg rewards for Q learning over SARSA across alphas, gammas, and epsilons. Sometimes, one did worse or better than the other, but the outcomes seem the same. Even in terms of average wait times, they were much more similar to each other compared to 2.i and 2.ii.\n",
    "\n",
    "\n",
    "## **2.c.iii Alpha VS Exploration VS GAMMA**\n",
    "For 2.iii, SARSA learning seemed much less prone to changing values. When controlling the alpha, gamma, and epsilon values, the avg wait times and rewards did not change much. However for Q learning, wehn we changed the values, we saw there was either a noticable increase or decrease in wait time.\n",
    "\n",
    "Overall, we see that the system at best preferred an agent that learned slowly , cared about immediate rewards, and chose other action 50% of the time over 13.89 hours for it to take an average of about 4 minutes for the elevator to arrive to a person's call floor and take them to their exit floor.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
