{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1.a**\n",
    "\n",
    "#### **State space**\n",
    "\n",
    "In a finite MPD, a **state space** is defined as all of the possible configurations the agent can be in given its environment. To create a state space we need to define all the pieces of important information to represent the MPD problem. I took into account the elevator, elevator floor, door status, call floors, exit floors, and location of the person. So we can say:\n",
    "\n",
    "$State\\_Space = $\n",
    "\n",
    "$((E_{A}, F_{A_i}, D_{A_k}), (E_{B}, F_{B_j}, D_{B_l}), ((F_{Call_m}, F_{Exit_o}, F_{Location_p}), (F_{Call_n}, F_{Exit_p}, F_{Location_r})))$ \n",
    "\n",
    "for all $i,j,k,l,m,n,o,p,q,r$\n",
    "\n",
    "Where:\n",
    "- $E$ is one of the elevators\n",
    "- $F$ is the floor of one of the elevators\n",
    "- $Door$ is the status of the door (OPEN or CLOSED)\n",
    "- $F_{Call}$ is the floor a person is calling from (0 if no passenger)\n",
    "- $F_{Exit}$ is the floor a person wants to exit (0 if no passenger)\n",
    "- $F_{Location}$ is where the person is located (IN_A, IN_B, or WAITING)\n",
    "- $i,j \\space \\epsilon \\space [1,2,3,4,5,6]$\n",
    "- $m,n,o,p \\space \\epsilon \\space [0,1,2,3,4,5,6]$ \n",
    "- $k,l \\space \\epsilon \\space [open, closed]$\n",
    "- $q,r \\space \\epsilon \\space [in\\_a, in\\_b, waiting]$\n",
    "\n",
    "The state space would be of size:\n",
    "\n",
    "$|State\\_Space| = |((1, 6, 2), (1, 6, 2), ((7, 7, 3), (7, 7, 3)))| = $\n",
    "\n",
    "$= |1 * 6 *  2 * 1 * 6 * 2 * 7 * 7 * 3 * 7 * 7 * 3| = $\n",
    "\n",
    "$= 3111696$ states\n",
    "\n",
    "#### **Action space**\n",
    "\n",
    "Simarly we define the **action space** as the unique actions that can be taken at a time step. \n",
    "Given there are 2 elevators, each elevator moves independently, and we have the action set for elevator as \n",
    "${UP, DOWN, HOLD, DOORS}$ we can define:\n",
    "\n",
    "$Action\\_Set = ['UP', 'DOWN', 'HOLD', 'DOORS']$ \n",
    "\n",
    "$Action\\_Space = ((Action\\_Set_{i} , E_{A}), (Action\\_Set_{j} , E_{B}))$\n",
    "\n",
    "for all $i,j$\n",
    "\n",
    "Where:\n",
    "- $E_{A}$ is elevator A\n",
    "- $E_{B}$ is elevator B\n",
    "\n",
    "The size of the action space is $(4 * 1 * 4 * 1) = 16 $\n",
    "\n",
    "#### **Time step**\n",
    "\n",
    "Lastly, we need to define a timestep. Since the elevator can only make decisions every 5 seconds, we can decide our timestep to be 5 seconds. And this timestep still holds even given the arrival rates, as we can model the arrivals rate by the number of seconds in the timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1.b**\n",
    "\n",
    "\n",
    "#### **Reward Function**\n",
    "\n",
    "For the reward function we care about the wait times. We need to devise a way to reward the agent when it minimizes the wait time of an action and penalize it when it lengthens the wait time of a passenger. We can define the reward psuedocode function as follows:\n",
    "\n",
    "```\n",
    "def reward(s, a, s')\n",
    "\n",
    "    reward = 0\n",
    "    m_r = movment reward\n",
    "    m_p = movement penalty\n",
    "\n",
    "    d_r = door reward\n",
    "    d_p = door penalty\n",
    "\n",
    "    h_r = hold reward\n",
    "    h_p = hold penalty\n",
    "\n",
    "    given action a, current state, and new state:\n",
    "\n",
    "        if closer to exit floor:\n",
    "            reward += m_r\n",
    "        else:\n",
    "            penalize -= m_p\n",
    "\n",
    "        if closer to call floor\n",
    "            reward += m_r\n",
    "        else:\n",
    "            penalize -= m_p\n",
    "\n",
    "        if closer doors opened and departed passenger:\n",
    "            reward += d_r\n",
    "        else:\n",
    "            penalize -= d_p\n",
    "\n",
    "        if hold doors and departed passengers\n",
    "            reward += h_r\n",
    "        else:\n",
    "            penalize -= h_p\n",
    "\n",
    "    return reward\n",
    "\n",
    "```\n",
    "This function would go through the action pairs for elevator A and B and aggregate the rewards for the corresponding action.\n",
    "\n",
    "#### **Utility**\n",
    "\n",
    "Additionally, generally, we typically define **Utility** as the expected sum of future rewards:\n",
    "\n",
    "$Utility = reward(s, a, s')_{t} + \\gamma_{k} * reward(s, a, s')_{t+1} +  ... + \\gamma^k * reward(s, a, s')_{t + n}$\n",
    "\n",
    "Where gamma is the discount factor. \n",
    "\n",
    "In terms of actual implementation of the **Utility function** in our MPD problem, we implicity define the utility of the state-actions inside a **Q table**, which is the expected future rewards of each state action pairs:\n",
    "\n",
    "$Q[state][action] = Utility(s,a)$\n",
    "\n",
    "Where $Utility(s,a)$ is a method to determine the expected future/expected rewards. The implementation of $Utility(s,a)$ would cahnge given on which RL algorithm being implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1.c**\n",
    "\n",
    "To define a simulator for the elevator system, we need to define:\n",
    "- a state transition rules for each step\n",
    "- a simulation for passengers\n",
    "- a reward function\n",
    "\n",
    "You can view the environment model / simulator implementation [HERE](./ENVIRONMENT/environment.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Solution 1.d**\n",
    "\n",
    "If we wanted to model impatience, we would have to incorporate **elapsed time** to our **state space** and **reward function** (the action space, given my implementation should be unchanged).\n",
    "\n",
    "- For our state space, we would liekly have to keep track of the time elapsed. Thw downside to this approach is that time is continous, and would likely exponentially increase the state space. Even if you discretized time, the state space would still be large.\n",
    "\n",
    "- For our reward function, we would have to use the elapsed time we keep track of in the state space and use that to add to the aggregate rewards for a single step. we would likely change the time to a negative number and add that to the rewards to incentivize shorter times: $reward= -T_{time} + reward$\n",
    "\n",
    "- For our action space, it should remain unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 1 MPD**\n",
    "\n",
    "Putting all our definition together we define our elevator MPD as :\n",
    "\n",
    "$MPD(S, A, T, R, S_{0}) = $\n",
    "- $s_a \\space \\epsilon \\space S \\space where \\space s_a = ((E_{A}, F_{A_i}, D_{A_k}), (E_{B}, F_{B_j}, D_{B_l}), ((F_{Call_m}, F_{Exit_o}, F_{Location_p}), (F_{Call_n}, F_{Exit_p}, F_{Location_r})))$\n",
    "- $a_a \\space \\epsilon \\space A $ where $ a_a \\space = ((Action\\_Set_{i} , E_{A}), (Action\\_Set_{j} , E_{B}))$\n",
    "- $T(s, a, s') = P(f_{e} | f_{c})$\n",
    "- $R = R(s, a, a')$\n",
    "- $S_{0} = s_{a_0} = ((A, 1, 1), (B, 1, 1), ((0, 0, waiting), (0, 0, waiting)))$ "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
