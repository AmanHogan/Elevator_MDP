{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.a.i - 3.a.iii**\n",
    "\n",
    "Q-learning-lambda has been implemented and the outputs for 2.i - 2.iii can be viewed [HERE](./sample_output/q_lambda/)\n",
    "\n",
    "The lambda algorithms take a very long time to complete. You can modify the number of iterations or modify the state space size to shorten the time in globals.py\n",
    "\n",
    "\n",
    "Unfotunately, the lambda algorithms require iterating through a large state space for each step. When using the original problems, it would take nearly a minute to move on step. So you can reduce the state space if you want\n",
    "as long as you change these variables in globals.py:\n",
    "```\n",
    "NFLOORS = 3\n",
    "START_FLOORS = [1]\n",
    "START_PROB = [1]\n",
    "EXIT_FLOORS = [2,3]\n",
    "EXIT_PROB = [.5, .5]\n",
    "FLOORS = [1, 2, 3]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.b.i - 3.b.iii**\n",
    "\n",
    "SARSA-lambda has been implemented and the outputs for 2.i - 2.iii can be viewed [here](./sample_output/q_sarsa_lambda/)\n",
    "\n",
    "Unfotunately, the lambda algorithms require iterating through a large state space for each step. When using the original problems, it would take nearly a minute to move on step. So you can reduce the state space if you want\n",
    "as long as you change these variables in globals.py:\n",
    "\n",
    "```\n",
    "NFLOORS = 3\n",
    "START_FLOORS = [1]\n",
    "START_PROB = [1]\n",
    "EXIT_FLOORS = [2,3]\n",
    "EXIT_PROB = [.5, .5]\n",
    "FLOORS = [1, 2, 3]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.c.i - 3.c.iii**\n",
    "\n",
    "### **The issue with Eligibility traces for this problem**\n",
    "The eligbility traces absolutely tanked the learning performance. To elucidate, for $Sarsa(\\lambda)$ or $Q(\\lambda)$ learning, we initialize an eligibility trace that is the same size as the Q table to keep a recird of the occurence of the state-action pairs:\n",
    "\n",
    "$|e(s,a)| = |Q(s,a)|$\n",
    "\n",
    "And recall, given our state space and action space, we have a Q table with a large amount of state-action pairs:\n",
    "\n",
    "$|Q\\_table| = |State\\_space| * |Action\\_space| = 3111696 * 16 = 49,787,136$\n",
    "\n",
    "So this means we have two tables of size 49,787,136. This isnt a problem alone with initialization or updates, since disctionaries in python have a fast lookup speed, however, the issue lies in the latter half of the $\\lambda$ algorithms:\n",
    "\n",
    "```\n",
    "for all s\n",
    "    for all a\n",
    "        update Q\n",
    "        update E\n",
    "```\n",
    "\n",
    "This requires us to loop through the entire set of state-action pairs, which is of size 49,787,136, for every step just to update our Q and Eligibility tables. For my PC, this tooknabout 30 seconds each step, making my agent learning, given the state space size, practically impossisble. \n",
    "\n",
    "### **Possible Solutions?**\n",
    "One way would be to reduce the size fo the state space. I made it possible for the user to modify the problem desctiption so they could use less floors if they wanted to for problems 2.i and 2.ii in the globals.py file:\n",
    "\n",
    "```\n",
    "NFLOORS = 3\n",
    "START_FLOORS = [1]\n",
    "START_PROB = [1]\n",
    "EXIT_FLOORS = [2,3]\n",
    "EXIT_PROB = [.5, .5]\n",
    "FLOORS = [1, 2, 3]\n",
    "```\n",
    "This would reduce the problem to 3 floors instead of 6, which would make the learning 37 x faster. For 2.iii, you would need to modify environment code beyond on top of modifying the globals.py\n",
    "\n",
    "\n",
    "You could also use helper classes to keep track of state information rather than using the state space. For example, instead of keeping track of passengers in the satte space, you could just make a class, which is just as valid, as long as you define this class mathematically to be apart of you state space.\n",
    "\n",
    "### **Algorithm Learning Curves**\n",
    "\n",
    "To get any useful inforamtion, I chose to reduce the state space size and reduce the number of iterations. Here is the alpha rewards graph for 2.i for Q lambda and SARSA lambda, respectively:\n",
    "\n",
    "<img src=\"./sample_output/q_lambda/avg_rewards_alpha_2i.png\" width=\"400\"/>\n",
    "<img src=\"./sample_output/sarsa_lambda/avg_rewards_alpha_2i.png\" width=\"400\"/>\n",
    "\n",
    "\n",
    "In conclusion, I wouldnt recommend using the lambda algorithms for large states spaces unless you have used methods to reduce the effect of the curse of dimensionality, otherwise, your learning time will be substantionally increased."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
